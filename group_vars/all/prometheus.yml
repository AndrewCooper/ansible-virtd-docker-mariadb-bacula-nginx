---
# Custom variables for prometheus

prometheus_version: v2.17.2
prometheus_image: 'quay.io/prometheus/prometheus:{{ prometheus_version }}'
# prometheus_container: '{{ docker_name_prefix }}prometheus'
prometheus_restart_policy: '{{ docker_restart_policy }}'

# prometheus_config_dir: /etc/prometheus
prometheus_etc_volume: '{{ docker_restore_config_base_dir }}/prometheus'

# prometheus_db_dir: /prometheus
prometheus_storage_volume: '{{ docker_name_prefix }}prometheus-data'

# prometheus_volumes:
# - '{{ prometheus_etc_volume }}:{{ prometheus_config_dir }}'
# - '{{ prometheus_storage_volume }}:{{ prometheus_db_dir }}'

# prometheus_port: 9090
# prometheus_web_listen_address: "0.0.0.0:{{ prometheus_port }}"
prometheus_web_external_url: 'https://{{ prometheus_hostname }}'
# prometheus_hostname: "prometheus.local"
prometheus_networks:
- name: '{{ docker_network_frontend }}'
  aliases:
  - '{{ prometheus_container }}'
  - '{{ prometheus_hostname }}'
# prometheus_port_args:
#   - '{{ prometheus_port }}'

# prometheus_storage_retention: "30d"
# Available since Prometheus 2.7.0
# [EXPERIMENTAL] Maximum number of bytes that can be stored for blocks. Units
# supported: KB, MB, GB, TB, PB.
# prometheus_storage_retention_size: "0"

# prometheus_config_flags:
# - --config.file={{ prometheus_config_dir }}/prometheus.yml
# - --storage.tsdb.path={{ prometheus_db_dir }}
# - --storage.tsdb.retention.time={{ prometheus_storage_retention }}
# - --storage.tsdb.retention.size={{ prometheus_storage_retention_size }}
# - --web.console.libraries=/usr/share/prometheus/console_libraries
# - --web.console.templates=/usr/share/prometheus/consoles
# - --web.listen-address={{ prometheus_web_listen_address }}
# - --web.external-url={{ prometheus_web_external_url }}
# - '{{ prometheus_config_flags_extra }}'

# prometheus_config_flags_extra: {}
# prometheus_config_flags_extra:
# - --storage.tsdb.retention=15d
# - --alertmanager.timeout=10s

# prometheus_alertmanager_config: []
# prometheus_alertmanager_config:
#   - scheme: https
#     path_prefix: /alertmanager
#     basic_auth:
#       username: user
#       password: pass
#     static_configs:
#       - targets: ["127.0.0.1:9093"]
#     proxy_url: "127.0.0.2"

# prometheus_alert_relabel_configs: []
# prometheus_alert_relabel_configs:
#   - action: labeldrop
#     regex: replica

# prometheus_global:
#   scrape_interval: 15s
#   scrape_timeout: 10s
#   evaluation_interval: 15s

# prometheus_remote_write: []
# prometheus_remote_write:
#   - url: https://dev.kausal.co/prom/push
#     basic_auth:
#       password: FOO

# prometheus_remote_read: []
# prometheus_remote_read:
#   - url: https://demo.cloudalchemy.org:9201/read
#     basic_auth:
#       password: FOO

prometheus_external_labels:
  environment: "{{ prometheus_hostname }}"

# prometheus_targets: {}
#  node:
#    - targets:
#        - localhost:9100
#      labels:
#        env: test

prometheus_scrape_configs:
- job_name: "prometheus"
  static_configs:
  - targets:
    - localhost:{{ prometheus_port }}
- job_name: "daedalus"
  static_configs:
  - targets:
    - 172.16.0.100:9100
- job_name: "testdaedalus"
  static_configs:
  - targets:
    - 172.16.0.200:9100
- job_name: "threadripper"
  static_configs:
  - targets:
    - 172.16.64.4:9100
- job_name: gitlab_redis
  static_configs:
  - targets:
    - "{{ gitlab_container_name }}:9121"
- job_name: gitlab_postgres
  static_configs:
  - targets:
    - "{{ gitlab_container_name }}:9187"
- job_name: gitlab_workhorse
  static_configs:
  - targets:
    - "{{ gitlab_container_name }}:9229"
- job_name: gitlab_unicorn
  metrics_path: "/-/metrics"
  static_configs:
  - targets:
    - "{{ gitlab_container_name }}:8080"
- job_name: gitlab_sidekiq
  static_configs:
  - targets:
    - "{{ gitlab_container_name }}:8082"
- job_name: gitlab_monitor_database
  metrics_path: "/database"
  static_configs:
  - targets:
    - "{{ gitlab_container_name }}:9168"
- job_name: gitlab_monitor_sidekiq
  metrics_path: "/sidekiq"
  static_configs:
  - targets:
    - "{{ gitlab_container_name }}:9168"
- job_name: gitlab_monitor_process
  metrics_path: "/process"
  static_configs:
  - targets:
    - "{{ gitlab_container_name }}:9168"
- job_name: gitaly
  static_configs:
  - targets:
    - "{{ gitlab_container_name }}:9236"
- job_name: grafana
  static_configs:
  - targets:
    - "{{ grafana_container }}:{{ grafana_port }}"
- job_name: loki
  static_configs:
  - targets:
    - "{{ loki_container }}:{{ loki_port_listen }}"
- job_name: promtail
  static_configs:
  - targets:
    - "{{ promtail_container }}:{{ promtail_port_listen }}"
# https://github.com/Lusitaniae/apache_exporter
- job_name: TestStation_http_server
  static_configs:
  - targets:
    - "{{teststation_http_monitor_container_name}}:9117"
# https://github.com/Lusitaniae/apache_exporter
- job_name: mantisbt
  static_configs:
  - targets:
    - "{{mantisbt_monitor_container_name}}:9117"
# https://github.com/Lusitaniae/apache_exporter
- job_name: wiki
  static_configs:
  - targets:
    - "{{wiki_monitor_container_name}}:9117"
# https://github.com/ocadotechnology/nexus-exporter
- job_name: nexus3
  static_configs:
  - targets:
    - "{{nexus_monitor_container_name}}:{{nexus_monitor_port}}"

# Alternative config file name, searched in ansible templates path.
# prometheus_config_file: 'prometheus.yml.j2'

# prometheus_alert_rules_files:
#   - prometheus/rules/*.rules

# prometheus_static_targets_files:
#   - prometheus/targets/*.yml
#   - prometheus/targets/*.json

# prometheus_alert_rules:
#   - alert: Watchdog
#     expr: vector(1)
#     for: 10m
#     labels:
#       severity: warning
#     annotations:
#       description: 'This is an alert meant to ensure that the entire alerting pipeline is functional.
#         This alert is always firing, therefore it should always be firing in Alertmanager
#         and always fire against a receiver. There are integrations with various notification
#         mechanisms that send a notification when this alert is not firing. For example the
#         "DeadMansSnitch" integration in PagerDuty.'
#       summary: 'Ensure entire alerting pipeline is functional'
#   - alert: InstanceDown
#     expr: "up == 0"
#     for: 5m
#     labels:
#       severity: critical
#     annotations:
#       description: "{% raw %}{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.{% endraw %}"
#       summary: "{% raw %}Instance {{ $labels.instance }} down{% endraw %}"
#   - alert: CriticalCPULoad
#     expr: '100 - (avg by (instance) (irate(node_cpu_seconds_total{job="node",mode="idle"}[5m])) * 100) > 96'
#     for: 2m
#     labels:
#       severity: critical
#     annotations:
#       description: "{% raw %}{{ $labels.instance }} of job {{ $labels.job }} has Critical CPU load for more than 2 minutes.{% endraw %}"
#       summary: "{% raw %}Instance {{ $labels.instance }} - Critical CPU load{% endraw %}"
#   - alert: CriticalRAMUsage
#     expr: '(1 - ((node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes) / node_memory_MemTotal_bytes)) * 100 > 98'
#     for: 5m
#     labels:
#       severity: critical
#     annotations:
#       description: "{% raw %}{{ $labels.instance }} has Critical Memory Usage more than 5 minutes.{% endraw %}"
#       summary: "{% raw %}Instance {{ $labels.instance }} has Critical Memory Usage{% endraw %}"
#   - alert: CriticalDiskSpace
#     expr: 'node_filesystem_free_bytes{mountpoint!~"^/run(/.*|$)",fstype!~"(squashfs|fuse.*)",job="node"} / node_filesystem_size_bytes{job="node"} < 0.1'
#     for: 4m
#     labels:
#       severity: critical
#     annotations:
#       description: "{% raw %}{{ $labels.instance }} of job {{ $labels.job }} has less than 10% space remaining.{% endraw %}"
#       summary: "{% raw %}Instance {{ $labels.instance }} - Critical disk space usage{% endraw %}"
#   - alert: RebootRequired
#     expr: "node_reboot_required > 0"
#     labels:
#       severity: warning
#     annotations:
#       description: "{% raw %}{{ $labels.instance }} requires a reboot.{% endraw %}"
#       summary: "{% raw %}Instance {{ $labels.instance }} - reboot required{% endraw %}"
#   - alert: ClockSkewDetected
#     expr: 'abs(node_timex_offset_seconds) * 1000 > 30'
#     for: 2m
#     labels:
#       severity: warning
#     annotations:
#       description: "{% raw %}Clock skew detected on {{ $labels.instance }}. Ensure NTP is configured correctly on this host.{% endraw %}"
#       summary: "{% raw %}Instance {{ $labels.instance }} - Clock skew detected{% endraw %}"
